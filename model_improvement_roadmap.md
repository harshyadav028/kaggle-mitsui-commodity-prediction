# ðŸš€ Model Improvement Roadmap

Weâ€™ve already built a **solid feature set**:
- âœ… Lagged log returns  
- âœ… Highâ€“low volatility spreads  
- âœ… Volume / open interest dynamics  
- âœ… Rolling stats (trend + volatility)  
- âœ… FX log returns  

This is the **backbone most competitors rely on**.  
Now, the **low-hanging fruit** lies in *training & evaluation strategies*.  

---

## ðŸ“Œ Next Steps & Checkpoints

### ðŸ”¹ 1. Target Handling
- [ ] **Audit NaNs** across all targets  
- [ ] **Drop extremely sparse targets** (>70% missing)  
- [ ] **Focus training** on denser targets (more reliable signals)  

ðŸ‘‰ Benefit: Removes noise that drags down Sharpe ratio  

---

### ðŸ”¹ 2. Model Objectives
- [ ] Switch from `regression` â†’ `regression_l1` (better rank alignment)  
- [ ] Try `objective="huber"` for robustness to outliers  

ðŸ‘‰ Benefit: Optimizes closer to our Sharpe-like evaluation  

---

### ðŸ”¹ 3. Hyperparameter Tuning
- [ ] Tune key params with **Optuna**:
  - `num_leaves`, `max_depth` â†’ complexity  
  - `min_child_samples` â†’ overfitting control  
  - `subsample`, `colsample_bytree` â†’ randomness/generalization  
  - `reg_alpha`, `reg_lambda` â†’ regularization  

ðŸ‘‰ **Next concrete step**: Run Optuna on **`target_0`** and reuse best params across all  

---

### ðŸ”¹ 4. Cross-Validation Strategy
- [ ] Replace 3-fold `TimeSeriesSplit` with **walk-forward validation**  
  - Train on `[0:T]` â†’ validate on `[T+1:T+k]`  
  - Slide window forward  

ðŸ‘‰ Benefit: Mimics real trading, avoids lookahead bias  

---

### ðŸ”¹ 5. Multi-Target Training
- [ ] Test `MultiOutputRegressor(LGBMRegressor())`  
- [ ] Prototype a **shared-backbone neural net** (shared layers â†’ multiple output heads)  

ðŸ‘‰ Benefit: Learns **cross-target correlations**  

---

### ðŸ”¹ 6. Efficiency
- [ ] Parallelize training with `joblib` / multiprocessing  
- [ ] Experiment with multi-output NN (1 model instead of 400+)  

ðŸ‘‰ Benefit: Saves time, scales better  

---

## âš¡ Action Plan
âœ… **Today**: Run Optuna tuning loop on `target_0`  
â¬œ **Next**: Apply tuned params across all targets  
â¬œ **Then**: Upgrade CV strategy to walk-forward  
â¬œ **Later**: Explore multi-target NN for efficiency & correlation learning  

---

ðŸ“Š **Goal**: Improve Sharpe ratio by reducing noise, aligning training with evaluation, and scaling efficiently.  
